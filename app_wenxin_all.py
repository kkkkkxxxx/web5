import streamlit as st
import os
from typing import Dict, List, Any
import requests
import numpy as np
import re
import time
from duckduckgo_search import DDGS
from FlagEmbedding import BGEM3FlagModel
from openai import OpenAI

# å…¼å®¹äº‘ç«¯æ—  chromadb
try:
    import chromadb
    from chromadb.utils import embedding_functions
    chromadb_available = True
except ImportError:
    chromadb_available = False

# ========== FactChecker ç±» ==========
class FactChecker:
    def __init__(self, api_base: str, model: str, temperature: float, max_tokens: int):
        # æ–‡å¿ƒAPIé…ç½®
        self.api_key = "5862f79af437ff09b44c6da222a7a1fd1a0eed52"  # ä½ çš„è®¿é—®ä»¤ç‰Œ
        self.api_base = "https://aistudio.baidu.com/llm/lmapi/v3"
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.api_base
        )

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        try:
            self.embedding_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
        except Exception as e:
            st.error(f"åŠ è½½BGE-M3æ¨¡å‹é”™è¯¯: {str(e)}")
            self.embedding_model = None

        # åˆå§‹åŒ–Chromaæœ¬åœ°çŸ¥è¯†åº“ï¼ˆäº‘ç«¯æ— chromadbæ—¶è‡ªåŠ¨é™çº§ï¼‰
        if chromadb_available:
            try:
                self.chroma_client = chromadb.Client()
                self.collection = self.chroma_client.get_or_create_collection(
                    name="my-knowledge-base",
                    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name="BAAI/bge-m3")
                )
            except Exception as e:
                st.error(f"åŠ è½½Chromaé”™è¯¯: {str(e)}")
                self.collection = None
        else:
            self.collection = None

    def extract_claim(self, text: str) -> str:
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„å£°æ˜æå–åŠ©æ‰‹ã€‚åˆ†ææä¾›çš„æ–°é—»å¹¶æ€»ç»“å…¶ä¸­å¿ƒæ€æƒ³ã€‚å°†ä¸­å¿ƒæ€æƒ³æ ¼å¼åŒ–ä¸ºä¸€ä¸ªå€¼å¾—æ ¸æŸ¥çš„é™ˆè¿°ï¼Œå³å¯ä»¥ç‹¬ç«‹éªŒè¯çš„å£°æ˜å¹¶ä»¥ä¸­æ–‡å›ç­”ã€‚è¾“å‡ºæ ¼å¼:\nclaim: <claim>"
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ]
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.0,
                max_tokens=1000
            )
            claims_text = response.choices[0].message.content
            claims = re.findall(r'\d+\.\s+(.*?)(?=\n\d+\.|\Z)', claims_text, re.DOTALL)
            claims = [claim.strip() for claim in claims if claim.strip()]
            if not claims and claims_text.strip():
                claims = [line.strip() for line in claims_text.strip().split('\n') if line.strip()]
            return claims[0] if claims else text
        except Exception as e:
            st.error(f"æå–å£°æ˜é”™è¯¯: {str(e)}")
            return text

    def extract_keyinformation(self, text: str) -> str:
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„å£°æ˜æå–åŠ©æ‰‹ã€‚è¯·ä½ å¯¹ç”¨æˆ·è¾“å…¥æ‰§è¡Œä»¥ä¸‹å¤šå±‚çº§åˆ†æï¼š\n"
            "1.è¯†åˆ«æ–‡æœ¬ä¸­æ¶‰åŠçš„æ‰€æœ‰å®ä½“ï¼ˆäººç‰©/ç»„ç»‡/åœ°ç‚¹/æ—¶é—´ï¼‰åŠå…¶ç±»å‹\n"
            "2.ç¡®å®šæ ¸å¿ƒäº‹ä»¶ç±»å‹ï¼ˆæ”¿æ²»/ç»æµ/ç¤¾ä¼š/ç§‘æŠ€ç­‰ç±»åˆ«ï¼‰\n"
            "3.æå–å…³é”®ç‰¹å¾ï¼ˆæ¥æºå¯é æ€§/è¯æ®å¼ºåº¦/æ—¶æ•ˆæ€§ï¼‰\n"
            "4.æ€»ç»“ä¸­å¿ƒæ€æƒ³å¹¶è½¬åŒ–ä¸ºå¯éªŒè¯çš„å£°æ˜\n\n"
            "è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š\n"
            "claim: <å¯éªŒè¯çš„å®Œæ•´é™ˆè¿°>\n"
            "entities: <å®ä½“ç±»å‹1:å®ä½“åç§°1, å®ä½“ç±»å‹2:å®ä½“åç§°2...>\n"
            "event_type: <äº‹ä»¶ä¸»ç±»åˆ«>\n"
            "key_features: <ç‰¹å¾1:å€¼1, ç‰¹å¾2:å€¼2...>\n\n"
            "æ³¨æ„ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Š"
        )
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ]
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.0,
                max_tokens=1000
            )
            raw_output = response.choices[0].message.content
            claim_match = re.search(r'claim:\s*(.*?)(?=\n\S+:|\Z)', raw_output, re.DOTALL)
            entities_match = re.search(r'entities:\s*(.*?)(?=\n\S+:|\Z)', raw_output, re.DOTALL)
            event_type_match = re.search(r'event_type:\s*(.*?)(?=\n\S+:|\Z)', raw_output, re.DOTALL)
            key_features_match = re.search(r'key_features:\s*(.*?)(?=\n\S+:|\Z)', raw_output, re.DOTALL)
            claim = claim_match.group(1).strip() if claim_match else "æœªæå–åˆ°æœ‰æ•ˆå£°æ˜"
            entities = entities_match.group(1).strip() if entities_match else "æ— å®ä½“ä¿¡æ¯"
            event_type = event_type_match.group(1).strip() if event_type_match else "æœªçŸ¥äº‹ä»¶ç±»å‹"
            key_features = key_features_match.group(1).strip() if key_features_match else "æ— ç‰¹å¾ä¿¡æ¯"
            formatted_output = (
                f"claim: {claim}\n"
                f"entities: {entities}\n"
                f"event_type: {event_type}\n"
                f"key_features: {key_features}"
            )
            return formatted_output
        except Exception as e:
            st.error(f"æå–å…³é”®ä¿¡æ¯é”™è¯¯: {str(e)}")
            return text

    def search_local_knowledge(self, claim: str, top_k: int = 5) -> List[Dict[str, str]]:
        if not self.collection:
            return []
        try:
            results = self.collection.query(
                query_texts=[claim],
                n_results=top_k
            )
            evidence_docs = []
            for text, metadata in zip(results["documents"][0], results["metadatas"][0]):
                evidence_docs.append({
                    'title': metadata.get('title', ''),
                    'url': metadata.get('url', 'Local Knowledge Base'),
                    'snippet': text
                })
            return evidence_docs
        except Exception as e:
            st.error(f"æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢é”™è¯¯: {str(e)}")
            return []

    def search_evidence(self, claim: str, num_results: int = 5) -> List[Dict[str, str]]:
        try:
            ddgs = DDGS(timeout=60)
            results = list(ddgs.text(claim, max_results=num_results))
            external_evidence = []
            for result in results:
                external_evidence.append({
                    'title': result.get('title', ''),
                    'url': result.get('href', ''),
                    'snippet': result.get('body', '')
                })
            local_evidence = self.search_local_knowledge(claim, top_k=num_results)
            combined_evidence = external_evidence + local_evidence
            return combined_evidence
        except Exception as e:
            st.error(f"å¤–éƒ¨è¯æ®æ£€ç´¢é”™è¯¯: {str(e)}")
            return []

    def get_evidence_chunks(self, evidence_docs: List[Dict[str, str]], claim: str, chunk_size: int = 200, chunk_overlap: int = 50, top_k: int = 10) -> List[Dict[str, Any]]:
        if not self.embedding_model:
            return [{
                'text': "Evidence ranking unavailable - BGE-M3 model could not be loaded.",
                'source': "System",
                'similarity': 0.0
            }]
        if not evidence_docs:
            return [{
                'text': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è¯æ®ã€‚",
                'source': "ç³»ç»Ÿ",
                'similarity': 0.0
            }]
        try:
            all_chunks = []
            for doc in evidence_docs:
                all_chunks.append({
                    'text': doc['title'],
                    'source': doc['url'],
                })
                snippet = doc['snippet']
                if len(snippet) <= chunk_size:
                    all_chunks.append({
                        'text': snippet,
                        'source': doc['url'],
                    })
                else:
                    for i in range(0, len(snippet), chunk_size - chunk_overlap):
                        chunk_text = snippet[i:i + chunk_size]
                        if len(chunk_text) >= chunk_size // 2:
                            all_chunks.append({
                                'text': chunk_text,
                                'source': doc['url'],
                            })
            claim_embedding = self.embedding_model.encode(claim)['dense_vecs']
            chunk_texts = [chunk['text'] for chunk in all_chunks]
            chunk_embeddings = self.embedding_model.encode(chunk_texts)['dense_vecs']
            similarities = []
            for i, chunk_embedding in enumerate(chunk_embeddings):
                similarity = np.dot(claim_embedding, chunk_embedding) / (
                    np.linalg.norm(claim_embedding) * np.linalg.norm(chunk_embedding)
                )
                similarities.append(float(similarity))
            for i, similarity in enumerate(similarities):
                all_chunks[i]['similarity'] = similarity
            ranked_chunks = sorted(all_chunks, key=lambda x: x['similarity'], reverse=True)
            return ranked_chunks[:top_k]
        except Exception as e:
            st.error(f"è¯æ®ç›¸å…³æ€§åˆ†æé”™è¯¯: {str(e)}")
            return [{
                'text': f"è¯æ®ç›¸å…³æ€§åˆ†æé”™è¯¯: {str(e)}",
                'source': "System",
                'similarity': 0.0
            }]

    def evaluate_claim(self, keyinformation: str, claim: str, evidence_chunks: List[Dict[str, Any]]) -> Dict[str, str]:
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ–°é—»æ ¸æŸ¥åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§å¦‚ä¸‹åˆ†ææµç¨‹ï¼š\n"
            "1. æ¥æºå¯ä¿¡åº¦æ ¸æŸ¥ï¼ˆæƒé‡40%ï¼‰\n"
            "2. è¯æ®è´¨é‡è¯„ä¼°ï¼ˆæƒé‡40%ï¼‰\n"
            "3. é€»è¾‘å®Œæ•´æ€§å®¡æŸ¥ï¼ˆæƒé‡20%ï¼‰\n"
            "4. äºŒå…ƒåˆ¤å®šæ ‡å‡†ï¼šTRUEï¼ˆæ€»æƒé‡â‰¥70%ä¸”æ— æ ¸å¿ƒçŸ›ç›¾ï¼‰ï¼ŒFALSEï¼ˆå­˜åœ¨åè¯é“¾/å¤šå¤„é€»è¾‘æ¼æ´æˆ–å¤§éƒ¨åˆ†æ¥æºä¸ºä¸ªäººåšå®¢/åŒ¿åå¹³å°ï¼‰ï¼ŒPARTIALLY TRUEï¼ˆéƒ¨åˆ†è¯æ®æ”¯æŒä½†æœ‰ç–‘ç‚¹ï¼‰\n"
            "è¾“å‡ºæ ¼å¼ï¼š\n"
            "VERDICT: [TRUE|FALSE|PARTIALLY TRUE]\n"
            "REASONING: \n"
            "1. æ¥æºè¯„çº§\n"
            "2. å…³é”®è¯æ®\n"
            "3. é€»è¾‘ç¼ºé™·\n"
            "4. æ—¶æ•ˆæ€§éªŒè¯\n"
            "è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Šã€‚"
        )
        evidence_text = "\n\n".join([
            f"evidence {i+1} (relevance: {chunk['similarity']:.2f}):\n{chunk['text']}\nsource: {chunk['source']}"
            for i, chunk in enumerate(evidence_chunks)
        ])
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"keyinformation: {keyinformation}\n\ntext:{claim}\n\nevidence:\n{evidence_text}"}
            ]
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            result_text = response.choices[0].message.content
            verdict_match = re.search(r'\s*(TRUE|FALSE|PARTIALLY TRUE).*?$', result_text, re.IGNORECASE | re.MULTILINE)
            verdict = verdict_match.group(1) if verdict_match else "UNVERIFIABLE"
            reasoning_match = re.search(r'REASONING:\s*(.*)', result_text, re.DOTALL)
            reasoning = reasoning_match.group(1).strip() if reasoning_match else result_text
            return {
                "verdict": verdict,
                "reasoning": reasoning
            }
        except Exception as e:
            st.error(f"è¯„ä¼°å£°æ˜é”™è¯¯: {str(e)}")
            return {
                "verdict": "é”™è¯¯",
                "reasoning": f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            }

# ========== Streamlit å‰ç«¯ ==========
st.set_page_config(
    page_title="AIè™šå‡æ–°é—»æ£€æµ‹å™¨",
    page_icon="ğŸ”",
    layout="wide",
    menu_items={
        'Get Help': None,
        'Report a bug': None,
        'About': None
    }
)

st.title("AIè™šå‡æ–°é—»æ£€æµ‹å™¨")
st.markdown("""
æœ¬åº”ç”¨ç¨‹åºä½¿ç”¨æ–‡å¿ƒå¤§æ¨¡å‹APIéªŒè¯æ–°é—»é™ˆè¿°çš„å‡†ç¡®æ€§ã€‚
è¯·åœ¨ä¸‹æ–¹è¾“å…¥éœ€è¦æ ¸æŸ¥çš„æ–°é—»ï¼Œç³»ç»Ÿå°†æ£€ç´¢ç½‘ç»œè¯æ®è¿›è¡Œæ–°é—»æ ¸æŸ¥ï¼Œæ— ç½‘ç»œæ—¶å°†åªè¿›è¡Œæœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ã€‚
""")

with st.sidebar:
    st.header("é…ç½®")
    model_option = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        [
            "ernie-3.5-8k",
            "ernie-4.0-turbo-8k",
            "ernie-4.5-turbo-128k-preview"
        ],
        index=0,
        help="é€‰æ‹©æ–‡å¿ƒå¤§æ¨¡å‹"
    )
    with st.expander("é«˜çº§è®¾ç½®"):
        temperature = st.slider("æ¸©åº¦", min_value=0.0, max_value=1.0, value=0.0, step=0.1,
                               help="è¾ƒä½çš„å€¼ä½¿å“åº”æ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼ä½¿å“åº”æ›´å…·åˆ›é€ æ€§")
        max_tokens = st.slider("æœ€å¤§å“åº”é•¿åº¦", min_value=100, max_value=8000, value=1000, step=100,
                              help="å“åº”ä¸­çš„æœ€å¤§æ ‡è®°æ•°")
    st.divider()
    st.markdown("### å…³äº ###")
    st.markdown("è™šå‡æ–°é—»æ£€æµ‹å™¨:")
    st.markdown("1. ä»æ–°é—»ä¸­æå–æ ¸å¿ƒå£°æ˜")
    st.markdown("2. åœ¨ç½‘ç»œä¸Šå’Œæœ¬åœ°åº“æœç´¢è¯æ®")
    st.markdown("3. ä½¿ç”¨BGE-M3æŒ‰ç›¸å…³æ€§å¯¹è¯æ®è¿›è¡Œæ’å")
    st.markdown("4. åŸºäºè¯æ®æä¾›ç»“è®º")
    st.markdown("ä½¿ç”¨LLMã€Streamlitã€BGE-M3å’ŒRAGå¼€å‘ â¤ï¸â¤ï¸â¤ï¸")

if 'messages' not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

user_input = st.chat_input("è¯·åœ¨ä¸‹æ–¹è¾“å…¥éœ€è¦æ ¸æŸ¥çš„æ–°é—»...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    assistant_message = st.chat_message("assistant")
    claim_placeholder = assistant_message.empty()
    information_placeholder = assistant_message.empty()
    evidence_placeholder = assistant_message.empty()
    verdict_placeholder = assistant_message.empty()

    # åˆå§‹åŒ–FactChecker
    fact_checker = FactChecker(
        api_base="https://aistudio.baidu.com/llm/lmapi/v3",
        model=model_option,
        temperature=temperature,
        max_tokens=max_tokens
    )

    # ç¬¬1æ­¥ï¼šæå–å£°æ˜
    claim_placeholder.markdown("### ğŸ” æ­£åœ¨æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜...")
    claim = fact_checker.extract_claim(user_input)
    if "claim:" in claim.lower():
        claim = claim.split("claim:")[-1].strip()
    claim_placeholder.markdown(f"### ğŸ” æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜\n\n{claim}")

    # ç¬¬2æ­¥ï¼šæå–å…³é”®ä¿¡æ¯
    information_placeholder.markdown(f"### ğŸ” æ­£åœ¨æå–æ–°é—»çš„å…³é”®ä¿¡æ¯...")
    information = fact_checker.extract_keyinformation(user_input)
    information_placeholder.markdown(f"### ğŸ” æå–æ–°é—»çš„å…³é”®ä¿¡æ¯\n\n{information}")

    # ç¬¬3æ­¥ï¼šæœç´¢è¯æ®
    evidence_placeholder.markdown("### ğŸŒ æ­£åœ¨æœç´¢ç›¸å…³è¯æ®...")
    evidence_docs = fact_checker.search_evidence(claim)

    # ç¬¬4æ­¥ï¼šè·å–ç›¸å…³è¯æ®å—
    evidence_placeholder.markdown("### ğŸŒ æ­£åœ¨åˆ†æè¯æ®ç›¸å…³æ€§...")
    evidence_chunks = fact_checker.get_evidence_chunks(evidence_docs, claim)

    # æ˜¾ç¤ºè¯æ®ç»“æœ
    evidence_md = "### ğŸ”— è¯æ®æ¥æº\n\n"
    for j, chunk in enumerate(evidence_chunks):
        evidence_md += f"**[{j+1}]:**\n"
        evidence_md += f"{chunk['text']}\n"
        evidence_md += f"æ¥æº: {chunk['source']}\n\n"
    evidence_placeholder.markdown(evidence_md)

    # ç¬¬5æ­¥ï¼šè¯„ä¼°å£°æ˜
    verdict_placeholder.markdown("### âš–ï¸ æ­£åœ¨è¯„ä¼°å£°æ˜çœŸå®æ€§...")
    evaluation = fact_checker.evaluate_claim(information, user_input, evidence_chunks)

    verdict = evaluation["verdict"]
    if verdict.upper() == "TRUE":
        emoji = "âœ…"
        verdict_cn = "æ­£ç¡®"
    elif verdict.upper() == "FALSE":
        emoji = "âŒ"
        verdict_cn = "é”™è¯¯"
    elif verdict.upper() == "PARTIALLY TRUE":
        emoji = "âš ï¸"
        verdict_cn = "éƒ¨åˆ†æ­£ç¡®"
    else:
        emoji = "â“"
        verdict_cn = "æ— æ³•éªŒè¯"

    verdict_md = f"### {emoji} ç»“è®º: {verdict_cn}\n\n"
    verdict_md += f"### æ¨ç†è¿‡ç¨‹\n\n{evaluation['reasoning']}\n\n"
    verdict_placeholder.markdown(verdict_md)

    # æ•´åˆå®Œæ•´çš„å“åº”å†…å®¹ç”¨äºä¿å­˜åˆ°èŠå¤©å†å²
    full_response = f"""
### ğŸ” æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜

{claim}

---

{information}

---

{evidence_md}

---

{verdict_md}
"""
    st.session_state.messages.append({"role": "assistant", "content": full_response})