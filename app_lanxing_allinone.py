import sys
import pysqlite3
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
import streamlit as st
import os
from datetime import datetime
import uuid
import time
import requests
import re
import numpy as np
from typing import Dict, List, Any
from FlagEmbedding import BGEM3FlagModel
import chromadb
from chromadb.utils import embedding_functions
import random
import string
import hashlib
import hmac
import base64
import urllib.parse
from bs4 import BeautifulSoup

# ==================== è“å¿ƒAPIç­¾åå·¥å…· ====================
def gen_nonce(length=8):
    chars = string.ascii_lowercase + string.digits
    return ''.join([random.choice(chars) for _ in range(length)])

def gen_canonical_query_string(params):
    if params:
        escape_uri = urllib.parse.quote
        raw = []
        for k in sorted(params.keys()):
            tmp_tuple = (escape_uri(k), escape_uri(str(params[k])))
            raw.append(tmp_tuple)
        s = "&".join("=".join(kv) for kv in raw)
        return s
    else:
        return ''

def gen_signature(app_secret, signing_string):
    bytes_secret = app_secret.encode('utf-8')
    hash_obj = hmac.new(bytes_secret, signing_string, hashlib.sha256)
    bytes_sig = base64.b64encode(hash_obj.digest())
    signature = str(bytes_sig, encoding='utf-8')
    return signature

def gen_sign_headers(app_id, app_key, method, uri, query):
    method = str(method).upper()
    uri = uri
    timestamp = str(int(time.time()))
    app_id = app_id
    app_key = app_key
    nonce = gen_nonce()
    canonical_query_string = gen_canonical_query_string(query)
    signed_headers_string = 'x-ai-gateway-app-id:{}\nx-ai-gateway-timestamp:{}\n' \
                            'x-ai-gateway-nonce:{}'.format(app_id, timestamp, nonce)
    signing_string = '{}\n{}\n{}\n{}\n{}\n{}'.format(method,
                                                     uri,
                                                     canonical_query_string,
                                                     app_id,
                                                     timestamp,
                                                     signed_headers_string)
    signing_string = signing_string.encode('utf-8')
    signature = gen_signature(app_key, signing_string)
    return {
        'X-AI-GATEWAY-APP-ID': app_id,
        'X-AI-GATEWAY-TIMESTAMP': timestamp,
        'X-AI-GATEWAY-NONCE': nonce,
        'X-AI-GATEWAY-SIGNED-HEADERS': "x-ai-gateway-app-id;x-ai-gateway-timestamp;x-ai-gateway-nonce",
        'X-AI-GATEWAY-SIGNATURE': signature
    }

# ==================== FactCheckerç±» ====================
class FactChecker:
    def __init__(self, model: str, temperature: float, max_tokens: int):
        self.APP_ID = "2025468964"  # æ›¿æ¢ä¸ºä½ çš„ APP_ID
        self.APP_KEY = "TbdyfqgdVNiNFhUA"  # æ›¿æ¢ä¸ºä½ çš„ APP_KEY
        self.URI = "/vivogpt/completions"
        self.DOMAIN = "api-ai.vivo.com.cn"
        self.METHOD = "POST"

        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        try:
            self.embedding_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
        except Exception as e:
            st.error(f"åŠ è½½BGE-M3æ¨¡å‹é”™è¯¯: {str(e)}")
            self.embedding_model = None

        # åˆå§‹åŒ–Chromaæœ¬åœ°çŸ¥è¯†åº“
        try:
            self.chroma_client = chromadb.Client()
            self.collection = self.chroma_client.get_or_create_collection(
                name="my-knowledge-base",
                embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name="BAAI/bge-m3")
            )
        except Exception as e:
            st.error(f"åŠ è½½Chromaé”™è¯¯: {str(e)}")
            self.collection = None

    def _call_lanxin_api(self, prompt: str) -> str:
        request_id = str(uuid.uuid4())
        session_id = str(uuid.uuid4())
        params = {"requestId": request_id}
        data = {
            "prompt": prompt,
            "model": self.model,
            "sessionId": session_id,
            "extra": {
                "temperature": self.temperature,
                "maxTokens": self.max_tokens
            }
        }
        headers = gen_sign_headers(self.APP_ID, self.APP_KEY, self.METHOD, self.URI, params)
        headers["Content-Type"] = "application/json"
        url = f"https://{self.DOMAIN}{self.URI}"
        try:
            response = requests.post(url, json=data, headers=headers, params=params, timeout=60)
            response.raise_for_status()
            res_json = response.json()
            if res_json.get("code") == 0 and "data" in res_json:
                return res_json["data"].get("content", "")
            else:
                st.error(f"è“å¿ƒ API è¿”å›é”™è¯¯: {res_json}")
                return ""
        except Exception as e:
            st.error(f"è°ƒç”¨è“å¿ƒ API å¤±è´¥: {str(e)}")
            return ""

    def extract_claim(self, text: str) -> str:
        system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„å£°æ˜æå–åŠ©æ‰‹ã€‚åˆ†ææä¾›çš„æ–°é—»å¹¶æ€»ç»“å…¶ä¸­å¿ƒæ€æƒ³ã€‚
            å°†ä¸­å¿ƒæ€æƒ³æ ¼å¼åŒ–ä¸ºä¸€ä¸ªå€¼å¾—æ ¸æŸ¥çš„é™ˆè¿°ï¼Œå³å¯ä»¥ç‹¬ç«‹éªŒè¯çš„å£°æ˜å¹¶ä»¥ä¸­æ–‡å›ç­”ã€‚
            è¾“å‡ºæ ¼å¼:
            claim: <claim>
        """
        prompt = f"{system_prompt}\n\nç”¨æˆ·è¾“å…¥: {text}"
        claims_text = self._call_lanxin_api(prompt)
        claims = re.findall(r'\d+\.\s+(.*?)(?=\n\d+\.|\Z)', claims_text, re.DOTALL)
        claims = [claim.strip() for claim in claims if claim.strip()]
        if not claims and claims_text.strip():
            claims = [line.strip() for line in claims_text.strip().split('\n') if line.strip()]
        return claims[0] if claims else text

    def extract_keyinformation(self, text: str) -> str:
        system_prompt = """
       ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„å£°æ˜æå–åŠ©æ‰‹ã€‚è¯·ä½ å¯¹ç”¨æˆ·è¾“å…¥æ‰§è¡Œä»¥ä¸‹å¤šå±‚çº§åˆ†æï¼š
       1.è¯†åˆ«æ–‡æœ¬ä¸­æ¶‰åŠçš„æ‰€æœ‰å®ä½“ï¼ˆäººç‰©/ç»„ç»‡/åœ°ç‚¹/æ—¶é—´ï¼‰åŠå…¶ç±»å‹
       2.ç¡®å®šæ ¸å¿ƒäº‹ä»¶ç±»å‹ï¼ˆæ”¿æ²»/ç»æµ/ç¤¾ä¼š/ç§‘æŠ€ç­‰ç±»åˆ«ï¼‰
       3.æå–å…³é”®ç‰¹å¾ï¼ˆæ¥æºå¯é æ€§/è¯æ®å¼ºåº¦/æ—¶æ•ˆæ€§ï¼‰
       4.æ€»ç»“ä¸­å¿ƒæ€æƒ³å¹¶è½¬åŒ–ä¸ºå¯éªŒè¯çš„å£°æ˜

       è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
       claim: <å¯éªŒè¯çš„å®Œæ•´é™ˆè¿°>
       entities: <å®ä½“ç±»å‹1:å®ä½“åç§°1, å®ä½“ç±»å‹2:å®ä½“åç§°2...>
       event_type: <äº‹ä»¶ä¸»ç±»åˆ«>
       key_features: <ç‰¹å¾1:å€¼1, ç‰¹å¾2:å€¼2...>

       æ³¨æ„ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Š
       """
        prompt = f"{system_prompt}\n\nç”¨æˆ·è¾“å…¥: {text}"
        claims_text = self._call_lanxin_api(prompt)
        claim_match = re.search(r'claim:\s*(.*?)(?=\n\S+:|\Z)', claims_text, re.DOTALL)
        entities_match = re.search(r'entities:\s*(.*?)(?=\n\S+:|\Z)', claims_text, re.DOTALL)
        event_type_match = re.search(r'event_type:\s*(.*?)(?=\n\S+:|\Z)', claims_text, re.DOTALL)
        key_features_match = re.search(r'key_features:\s*(.*?)(?=\n\S+:|\Z)', claims_text, re.DOTALL)
        claim = claim_match.group(1).strip() if claim_match else "æœªæå–åˆ°æœ‰æ•ˆå£°æ˜"
        entities = entities_match.group(1).strip() if entities_match else "æ— å®ä½“ä¿¡æ¯"
        event_type = event_type_match.group(1).strip() if event_type_match else "æœªçŸ¥äº‹ä»¶ç±»å‹"
        key_features = key_features_match.group(1).strip() if key_features_match else "æ— ç‰¹å¾ä¿¡æ¯"
        formatted_output = (
            f"claim: {claim}\n"
            f"entities: {entities}\n"
            f"event_type: {event_type}\n"
            f"key_features: {key_features}"
        )
        return formatted_output

    def search_local_knowledge(self, claim: str, top_k: int = 5) -> List[Dict[str, str]]:
        if not self.collection:
            return []
        try:
            results = self.collection.query(
                query_texts=[claim],
                n_results=top_k
            )
            evidence_docs = []
            for text, metadata in zip(results["documents"][0], results["metadatas"][0]):
                evidence_docs.append({
                    'title': metadata.get('title', ''),
                    'url': metadata.get('url', 'Local Knowledge Base'),
                    'snippet': text
                })
            return evidence_docs
        except Exception as e:
            st.error(f"æœç´¢æœ¬åœ°çŸ¥è¯†åº“é”™è¯¯: {str(e)}")
            return []

    def search_baidu(self, query, num_results=5):
        """ç™¾åº¦æœç´¢æ‘˜è¦æŠ“å–"""
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        url = f"https://www.baidu.com/s?wd={urllib.parse.quote(query)}"
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(resp.text, "html.parser")
            results = []
            for idx, result in enumerate(soup.select(".result")[:num_results]):
                title = result.select_one("h3")
                title = title.get_text(strip=True) if title else ""
                snippet = result.select_one(".c-abstract") or result.select_one(".c-span-last")
                snippet = snippet.get_text(strip=True) if snippet else ""
                link = result.select_one("a")["href"] if result.select_one("a") else ""
                results.append({
                    "title": title,
                    "url": link,
                    "snippet": snippet
                })
            return results
        except Exception as e:
            st.warning(f"ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return []

    def search_evidence(self, claim: str, num_results: int = 5) -> List[Dict[str, str]]:
        evidence = []
        # DuckDuckGoï¼ˆå¤–ç½‘ï¼‰
        try:
            from duckduckgo_search import DDGS
            ddgs = DDGS(timeout=60)
            results = list(ddgs.text(claim, max_results=num_results))
            for result in results:
                evidence.append({
                    'title': result.get('title', ''),
                    'url': result.get('href', ''),
                    'snippet': result.get('body', '')
                })
        except Exception as e:
            st.warning(f"DuckDuckGoæœç´¢å¤±è´¥: {e}")

        # ç™¾åº¦ï¼ˆå›½å†…ï¼‰
        baidu_results = self.search_baidu(claim, num_results=num_results)
        evidence.extend(baidu_results)

        # æœ¬åœ°çŸ¥è¯†åº“
        local_evidence = self.search_local_knowledge(claim, top_k=num_results)
        evidence.extend(local_evidence)

        return evidence

    def get_evidence_chunks(self, evidence_docs: List[Dict[str, str]], claim: str, chunk_size: int = 200,
                            chunk_overlap: int = 50, top_k: int = 10) -> List[Dict[str, Any]]:
        if not self.embedding_model:
            return [{
                'text': "è¯æ®æ’åºä¸å¯ç”¨ - æ— æ³•åŠ è½½BGE-M3æ¨¡å‹",
                'source': "ç³»ç»Ÿ",
                'similarity': 0.0
            }]
        if not evidence_docs:
            return [{
                'text': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è¯æ®",
                'source': "ç³»ç»Ÿ",
                'similarity': 0.0
            }]
        try:
            all_chunks = []
            for doc in evidence_docs:
                all_chunks.append({
                    'text': doc['title'],
                    'source': doc['url'],
                })
                snippet = doc['snippet']
                if len(snippet) <= chunk_size:
                    all_chunks.append({
                        'text': snippet,
                        'source': doc['url'],
                    })
                else:
                    for i in range(0, len(snippet), chunk_size - chunk_overlap):
                        chunk_text = snippet[i:i + chunk_size]
                        if len(chunk_text) >= chunk_size // 2:
                            all_chunks.append({
                                'text': chunk_text,
                                'source': doc['url'],
                            })
            claim_embedding = self.embedding_model.encode(claim)['dense_vecs']
            chunk_texts = [chunk['text'] for chunk in all_chunks]
            chunk_embeddings = self.embedding_model.encode(chunk_texts)['dense_vecs']
            similarities = []
            for i, chunk_embedding in enumerate(chunk_embeddings):
                similarity = np.dot(claim_embedding, chunk_embedding) / (
                    np.linalg.norm(claim_embedding) * np.linalg.norm(chunk_embedding)
                )
                similarities.append(float(similarity))
            for i, similarity in enumerate(similarities):
                all_chunks[i]['similarity'] = similarity
            ranked_chunks = sorted(all_chunks, key=lambda x: x['similarity'], reverse=True)
            return ranked_chunks[:top_k]
        except Exception as e:
            st.error(f"è¯æ®æ’åºé”™è¯¯: {str(e)}")
            return [{
                'text': f"è¯æ®æ’åºé”™è¯¯: {str(e)}",
                'source': "ç³»ç»Ÿ",
                'similarity': 0.0
            }]

    def evaluate_claim(self, keyinformation: str, text: str, evidence_chunks: List[Dict[str, Any]]) -> Dict[str, str]:
        system_prompt = """
        ä»¥ä¸‹æ˜¯æ‚¨æä¾›çš„è‹±æ–‡å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

### 1. **æ¥æºå¯ä¿¡åº¦éªŒè¯ï¼ˆæƒé‡ï¼š40%ï¼‰**  
   âœ“ å®˜æ–¹æ¥æºï¼ˆæ”¿åºœ/å­¦æœ¯/ä¸“ä¸šæœŸåˆŠï¼‰ â†’ â˜…â˜…â˜…â˜…â˜…  
   âœ“ ä¸»æµåª’ä½“ï¼ˆæ³•æ–°ç¤¾/æ–°åç¤¾ç­‰ï¼‰ â†’ â˜…â˜…â˜…â˜†â˜†  
   âœ— ä¸ªäººåšå®¢/åŒ¿åå¹³å° â†’ â˜…â˜†â˜†â˜†â˜†  

### 2. **è¯æ®è´¨é‡è¯„ä¼°ï¼ˆæƒé‡ï¼š40%ï¼‰**  
   â”‚ ç›´æ¥è¯æ®          â”‚ é—´æ¥è¯æ®  
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
   â”‚ å®éªŒæ•°æ®          â”‚ ä¸“å®¶è¯è¯  
   â”‚ ç»Ÿè®¡æŠ¥å‘Š          â”‚ å†å²æ¡ˆä¾‹  
   â”‚ ç°åœºå½±åƒ          â”‚ æ–‡çŒ®å¼•ç”¨  

### 3. **é€»è¾‘å®Œæ•´æ€§å®¡æŸ¥ï¼ˆæƒé‡ï¼š20%ï¼‰**  
   - ç»Ÿè®¡é™·é˜±æ£€æµ‹ï¼šæ ·æœ¬é‡ <30  
   - å› æœè°¬è¯¯è¯†åˆ«ï¼šAâ†’B è¢«æ›²è§£ä¸º Bâ†’A  
   - æ—¶é—´æ‚–è®ºéªŒè¯ï¼šäº‹ä»¶é—´éš” <2å°æ—¶  

### 4. **äºŒå…ƒåˆ¤æ–­æ ‡å‡†**  
   **TRUE**: æ€»æƒé‡ â‰¥70% ä¸”æ— æ ¸å¿ƒçŸ›ç›¾  
   **FALSE**: å­˜åœ¨åè¯é“¾/å¤šé‡é€»è¾‘æ¼æ´ï¼Œæˆ–å¤šæ•°è¯æ®æ¥è‡ªä¸ªäººåšå®¢/åŒ¿åå¹³å°  

---

### **è¾“å‡ºè§„èŒƒ**  
**VERDICT**: [TRUE|FALSE|PARTIALLY TRUE]  
**EVIDENCE WEIGHT**: [æ€»è¯æ®æƒé‡ç™¾åˆ†æ¯”]  
**REASONING**:  
1. æ¥æºè¯„çº§ï¼šâ–¡å®˜æ–¹ â–¡è®¤è¯åª’ä½“ â–¡å¯ç–‘å¹³å°  
2. å…³é”®è¯æ®: [E-12] NASAé¥æ„Ÿæ•°æ® (2024Q3)  
3. é€»è¾‘æ¼æ´: æ£€æµ‹åˆ°æ ·æœ¬é€‰æ‹©åå·® (p=0.12)  
4. æ—¶é—´æœ‰æ•ˆæ€§: âœ“ æœ€æ–°å«æ˜Ÿå½±åƒ (2025-03)  

---

### **è¾“å…¥ç¤ºä¾‹**:  
"æœˆçƒèƒŒé¢å­˜åœ¨å¤–æ˜ŸäººåŸºåœ°"

### **è¾“å‡ºç¤ºä¾‹**:  
**VERDICT**: FALSE  
**EVIDENCE WEIGHT**: 92%  
**REASONING**:  
1. æ¥æºè¯„çº§: NASAå®˜æ–¹æ•°æ® (æƒé‡: 50%)  
2. å…³é”®è¯æ®: [E-3] æœˆçƒè½¦æ‰«ææ˜¾ç¤ºæ— å¼‚å¸¸ç»“æ„  
3. é€»è¾‘æ¼æ´: ç›®å‡»è€…è¯è¯å±äºè½¶äº‹è°¬è¯¯  
4. æ—¶é—´éªŒè¯: æœ€æ–°æœˆçƒä»»åŠ¡æ•°æ® (2024Q3)  

---

### è¯´æ˜ï¼š
- ä¿ç•™è‹±æ–‡æœ¯è¯­ï¼ˆå¦‚ VERDICTã€TRUE/FALSEï¼‰ä»¥ç¡®ä¿æ ¼å¼ç»Ÿä¸€ã€‚
- è¯æ®ç¼–å·ï¼ˆå¦‚ [E-3]ï¼‰å’Œæ•°æ®æ ¼å¼ï¼ˆå¦‚ 2024Q3ï¼‰ä¿æŒåŸæ ·ã€‚
- ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ï¼ˆå¦‚ `âœ“`ã€`â€¢`ï¼‰å¢å¼ºå¯è¯»æ€§ã€‚
        """
        evidence_text = "\n\n".join([
            f"è¯æ® {i+1} (ç›¸å…³æ€§: {chunk['similarity']:.2f}):\n{chunk['text']}\næ¥æº: {chunk['source']}"
            for i, chunk in enumerate(evidence_chunks)
        ])
        prompt = f"{system_prompt}\n\nå…³é”®ä¿¡æ¯: {keyinformation}\n\n{text}\n\nè¯æ®:\n{evidence_text}"
        result_text = self._call_lanxin_api(prompt)
        verdict_match = re.search(r'VERDICT:\s*(TRUE|FALSE)', result_text, re.IGNORECASE)
        verdict = verdict_match.group(1) if verdict_match else "UNVERIFIABLE"
        reasoning_match = re.search(r'REASONING:\s*(.*)', result_text, re.DOTALL)
        reasoning = reasoning_match.group(1).strip() if reasoning_match else result_text
        return {
            "verdict": verdict,
            "reasoning": reasoning
        }

# ==================== Streamlitå‰ç«¯ ====================
st.set_page_config(
    page_title="AIè™šå‡æ–°é—»æ£€æµ‹å™¨",
    page_icon="ğŸ”",
    layout="wide",
    menu_items={
        'Get Help': None,
        'Report a bug': None,
        'About': None
    }
)

# ====== ç§‘æŠ€æ„Ÿé¡µé¢ç¾åŒ–CSS ======
st.markdown("""
<style>
/* åŠ¨æ€æ¸å˜èƒŒæ™¯ */
body, [data-testid="stAppViewContainer"] {
    background: linear-gradient(120deg, #0f2027 0%, #2c5364 100%, #00eaff 200%) !important;
    animation: gradientBG 10s ease infinite;
    background-size: 200% 200%;
}
@keyframes gradientBG {
    0% {background-position: 0% 50%;}
    50% {background-position: 100% 50%;}
    100% {background-position: 0% 50%;}
}

/* ç»ç’ƒæ‹Ÿæ€å¡ç‰‡ */
.st-emotion-cache-1v0mbdj, .st-emotion-cache-1kyxreq, .stChatMessage {
    background: rgba(30, 41, 59, 0.7) !important;
    border-radius: 18px !important;
    box-shadow: 0 8px 32px 0 rgba(0,234,255,0.18);
    backdrop-filter: blur(8px);
    border: 1.5px solid rgba(0,234,255,0.18);
    padding: 1.8em 2.2em !important;
    margin-bottom: 1.7em !important;
    transition: box-shadow 0.3s;
}
.st-emotion-cache-1v0mbdj:hover, .st-emotion-cache-1kyxreq:hover, .stChatMessage:hover {
    box-shadow: 0 0 24px 4px #00eaff55;
}

/* ç§‘æŠ€æ„Ÿæ ‡é¢˜å­—ä½“å’Œå‘å…‰ */
h1, h2, h3, h4 {
    color: #00eaff !important;
    letter-spacing: 1.5px;
    font-family: 'Orbitron', 'Consolas', 'Arial', sans-serif;
    text-shadow: 0 0 8px #00eaff88, 0 0 2px #fff;
}

/* æŒ‰é’®åŠ¨æ•ˆ */
.stButton>button {
    background: linear-gradient(90deg, #00eaff 0%, #005bea 100%);
    color: #fff;
    border: none;
    border-radius: 10px;
    font-weight: bold;
    font-size: 1.1em;
    box-shadow: 0 0 12px #00eaff44;
    transition: 0.2s, box-shadow 0.3s;
}
.stButton>button:hover {
    background: linear-gradient(90deg, #005bea 0%, #00eaff 100%);
    color: #fff;
    box-shadow: 0 0 24px #00eaff;
    transform: scale(1.05);
}

/* æ»‘å—ç¾åŒ– */
.stSlider>div>div {
    background: #232526 !important;
    border-radius: 8px;
}

/* ç‚«å½©åˆ†å‰²çº¿ */
hr {
    border: none;
    border-top: 2.5px solid;
    border-image: linear-gradient(90deg, #00eaff, #005bea, #00eaff) 1;
    margin: 2em 0;
}

/* æ»šåŠ¨æ¡ */
::-webkit-scrollbar-thumb {
    background: linear-gradient(90deg, #00eaff 0%, #005bea 100%);
    border-radius: 8px;
}

/* èŠå¤©è¾“å…¥æ¡†ç¾åŒ– */
.stChatInputContainer {
    background: rgba(0,234,255,0.08) !important;
    border-radius: 12px !important;
    border: 1.5px solid #00eaff33 !important;
    box-shadow: 0 0 8px #00eaff22;
}

/* å¡ç‰‡å†…å®¹å­—ä½“ */
.stMarkdown, .stText, .stChatMessageContent {
    font-family: 'JetBrains Mono', 'Consolas', 'Arial', sans-serif;
    font-size: 1.08em;
    color: #e0e6ed;
}

/* å›¾æ ‡åŠ¨ç”» */
.icon-glow {
    filter: drop-shadow(0 0 8px #00eaffcc);
    animation: iconGlow 2s infinite alternate;
}
@keyframes iconGlow {
    0% { filter: drop-shadow(0 0 8px #00eaffcc);}
    100% { filter: drop-shadow(0 0 24px #00eaff);}
}
</style>
<link href="https://fonts.googleapis.com/css?family=Orbitron:700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=JetBrains+Mono:400,700&display=swap" rel="stylesheet">
""", unsafe_allow_html=True)

# ====== é¡µé¢å†…å®¹ ======
st.markdown("""
<div style="font-size:2.2em; font-family:Orbitron,Arial,sans-serif; color:#00eaff; text-align:center; margin-bottom:0.2em; letter-spacing:2px;">
    <span class="icon-glow">ğŸ¤–</span> AIè™šå‡æ–°é—»æ£€æµ‹å™¨
</div>
<div style="font-size:1.2em; color:#e0e6ed; background:rgba(0,234,255,0.10); border-radius:14px; padding:1.2em 2em; margin-bottom:1.5em; border:1.5px solid #00eaff33;">
    <b>æœ¬åº”ç”¨ç¨‹åºåŸºäº <span style="color:#00eaff;">è“å¿ƒå¤§æ¨¡å‹ï¼ˆvivo BlueLMï¼‰</span>ï¼Œèåˆå¤šæºè¯æ®ï¼Œæ™ºèƒ½éªŒè¯æ–°é—»çœŸä¼ªã€‚<br>
    <span style="color:#00eaff;">è¾“å…¥æ–°é—»å†…å®¹ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ£€ç´¢ç½‘ç»œä¸æœ¬åœ°çŸ¥è¯†åº“ï¼Œè¾“å‡ºæƒå¨æ ¸æŸ¥ç»“è®ºã€‚</span></b>
</div>
""", unsafe_allow_html=True)

with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    model_option = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        ["vivo-BlueLM-TB-Pro", "vivo-BlueLM-TB"],
        index=0,
        help="ä½¿ç”¨ vivo è“å¿ƒå¤§æ¨¡å‹"
    )
    with st.expander("é«˜çº§è®¾ç½®"):
        temperature = st.slider("æ¸©åº¦", min_value=0.0, max_value=1.0, value=0.7, step=0.1,
                               help="è¾ƒä½çš„å€¼ä½¿å“åº”æ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼ä½¿å“åº”æ›´å…·åˆ›é€ æ€§")
        max_tokens = st.slider("æœ€å¤§å“åº”é•¿åº¦", min_value=100, max_value=8000, value=1000, step=100,
                              help="å“åº”ä¸­çš„æœ€å¤§æ ‡è®°æ•°")
    st.markdown("""---""")
    st.markdown("""
    <div style="color:#00eaff; font-weight:bold;">å…³äº</div>
    <ul style="color:#e0e6ed;">
        <li>ğŸ”¹ ä»æ–°é—»ä¸­æå–æ ¸å¿ƒå£°æ˜</li>
        <li>ğŸ”¹ å¤šæºæ£€ç´¢ç½‘ç»œä¸æœ¬åœ°è¯æ®</li>
        <li>ğŸ”¹ BGE-M3æ¨¡å‹æ™ºèƒ½ç›¸å…³æ€§æ’åº</li>
        <li>ğŸ”¹ ä¾æ®è¯æ®è‡ªåŠ¨æ¨ç†ç»“è®º</li>
    </ul>
    <div style="color:#00eaff;">LLM + Streamlit + BGE-M3 + RAG</div>
    """, unsafe_allow_html=True)

if 'messages' not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

user_input = st.chat_input("è¯·åœ¨ä¸‹æ–¹è¾“å…¥éœ€è¦æ ¸æŸ¥çš„æ–°é—»...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    assistant_message = st.chat_message("assistant")
    claim_placeholder = assistant_message.empty()
    information_placeholder = assistant_message.empty()
    evidence_placeholder = assistant_message.empty()
    verdict_placeholder = assistant_message.empty()

    fact_checker = FactChecker(model=model_option, temperature=temperature, max_tokens=max_tokens)

    claim_placeholder.markdown("### ğŸ” æ­£åœ¨æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜...")
    claim = fact_checker.extract_claim(user_input)
    if "claim:" in claim.lower():
        claim = claim.split("claim:")[-1].strip()
    claim_placeholder.markdown(f"### ğŸ” æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜\n\n{claim}")

    information_placeholder.markdown(f"### ğŸ” æ­£åœ¨æå–æ–°é—»çš„å…³é”®ä¿¡æ¯...")
    information = fact_checker.extract_keyinformation(user_input)
    information_placeholder.markdown(f"### ğŸ” æå–æ–°é—»çš„å…³é”®ä¿¡æ¯\n\n{information}")

    evidence_placeholder.markdown("### ğŸŒ æ­£åœ¨æœç´¢ç›¸å…³è¯æ®...")
    evidence_docs = fact_checker.search_evidence(claim)

    evidence_placeholder.markdown("### ğŸŒ æ­£åœ¨åˆ†æè¯æ®ç›¸å…³æ€§...")
    evidence_chunks = fact_checker.get_evidence_chunks(evidence_docs, claim)

    evidence_md = "### ğŸ”— è¯æ®æ¥æº\n\n"
    for j, chunk in enumerate(evidence_chunks[:-1]):
        evidence_md += f"**[{j+1}]:**\n"
        evidence_md += f"{chunk['text']}\n"
        evidence_md += f"æ¥æº: {chunk['source']}\n\n"
    evidence_placeholder.markdown(evidence_md)

    verdict_placeholder.markdown("### âš–ï¸ æ­£åœ¨è¯„ä¼°å£°æ˜çœŸå®æ€§...")
    evaluation = fact_checker.evaluate_claim(information, user_input, evidence_chunks)

    verdict = evaluation["verdict"]
    if verdict.upper() == "TRUE":
        emoji = "âœ…"
        verdict_cn = "æ­£ç¡®"
    elif verdict.upper() == "FALSE":
        emoji = "âŒ"
        verdict_cn = "é”™è¯¯"
    elif verdict.upper() == "PARTIALLY TRUE":
        emoji = "âš ï¸"
        verdict_cn = "éƒ¨åˆ†æ­£ç¡®"
    else:
        emoji = "â“"
        verdict_cn = "æ— æ³•éªŒè¯"

    verdict_md = f"### {emoji} ç»“è®º: {verdict_cn}\n\n"
    verdict_md += f"### æ¨ç†è¿‡ç¨‹\n\n{evaluation['reasoning']}\n\n"
    verdict_placeholder.markdown(verdict_md)

    full_response = f"""
### ğŸ” æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜

{claim}

---

{information}

---

{evidence_md}

---

{verdict_md}
"""
    st.session_state.messages.append({"role": "assistant", "content": full_response})